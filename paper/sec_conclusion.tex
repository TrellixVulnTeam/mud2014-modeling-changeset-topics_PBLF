% vim:syntax=tex

In this paper conducted an exploratory study on modeling the topics of
changesets.
We used latent Dirichlet allocation (LDA) to extract linguistic
topics from changesets and snapshots (releases).

We addressed two research questions regarding the topic modeling of changesets.
First, we investigated whether changeset copora were any different than
traditional snapshot corpora, and what differences there might be.
For two of the systems, we found that the changeset vocabulary was a superset
to the snapshot vocabulary.
We measured the cosine distance of each distribution of words,
and found for 3 of the systems low (between 0.003 to 0.07),
while the last was much higher than the others (over 0.33).
Next, we investigated whether a topic model trained on a changeset corpus
was more or less distinct than a topic model trained on a snapshot corpus.
For 2 of the 4 systems, we found that the changeset corpus produced more
distinct topics, while for the other 2 it did not.


Future work includes expanding our evaluation and conducting an experiment
where we utilize these topic models, such as for bug localization.
Additional future work includes expanding our study to other systems, particularly ones that are not Java.
It seems unlikely that our results are specific to Java systems, though we cannot confirm this assumption without experimentation.
This expansion should also include an investigation into why some
changeset topic models are more distinct than others.
