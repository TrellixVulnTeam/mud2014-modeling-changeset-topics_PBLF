% vim:syntax=tex

In this section we describe the design of a case study in which we
explore the relationship between ownership and linguistic topics in source code.
We describe the case study using the Goal-Question-Metric approach~\cite{Basili-etal:94}.
%The data for the case study is available in this paper's online
%appendix\footnote{\url{http://software.eng.ua.edu/data/ownership-topics}}.

\subsection{Definition and Context}

Our \textit{goal} is to explore the relationship between changeset topics and snapshot topics.
The \textit{quality focus} of the study is on informing development decisions and policy changes
that could lead to software with fewer defects.
The \textit{perspective} of the study is of a researcher, developer, or project manager who wishes
to gain understanding of the concepts or features implemented in the source code.
The \textit{context} of the study spans the version histories of four open source systems.

Toward achievement of our goal, we pose the following research questions:
\begin{enumerate}
    \item[]\hspace*{-20pt}\textit{RQ1: Do changeset- and snapshot-based corpora express the same terms?}
    \item[]\hspace*{-20pt}\textit{RQ2: Are topic models trained on changesets more distinct than topic models trained on a snapshot?}
\end{enumerate}
Basically, we want to know whether topic modeling changesets is as good as, or better than, topic modeling a snapshot.

In the remainder of this section we introduce the subjects of our study,
describe the setting of our study, and report our data collection and analysis procedures.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Subject software systems}

The four subjects of our study ---
Apache \ant\footnote{\url{http://ant.apache.org/}},
\aspectj\footnote{\url{http://eclipse.org/aspectj/}},
\jodatime\footnote{\url{http://www.joda.org/joda-time/}},
and \postgres\footnote{\url{http://www.postgresql.org/}}
--- vary in language, size and application domain.
\ant\ is a library and command-line tool for managing builds,
\aspectj\ is an aspect-oriented programming extension for the Java language,
\jodatime\ is a library for replacing the Java standard library \texttt{date} and \texttt{time} classes,
and \postgres\ is an object-relational database management system.
Each system is stored in a Git repository, and the developers of each system use descriptive commit messages.
Further, the developers store bug reports in an issue tracker.
All systems are written in Java with the exception of \postgres,
which is written in C.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Setting}

To conduct the studies, we instantiate the process described in Section~\ref{sec:related} and illustrated in Figure~\ref{fig:process}.

\begin{comment}
"Traditional metrics are, indeed, negatively correlated with the
measures of topic quality developed in this paper.  Our measures enable
new forms of model selection and suggest that practitioners developing
topic models should thus focus on evaluations that depend on real-world
task performance rather than optimizing likelihood-based measures." ~\cite{Chang-etal:2009}
\end{comment}

We implemented our document extractor in Python v2.7
using the Dulwich library\footnote{\url{http://www.samba.org/~jelmer/dulwich/}} %\footnote{\url{https://pypi.python.org/pypi/dulwich}}
We extract documents from both a snapshot of the repository at a tagged
release and each commit reachable from that tag's commit.
The same preprocessing steps are employed on all documents extracted.

For our document extraction from a release,
we simply use the entire contents of the document.
Unlike previous methods\needcite,
we do not parse the source code documents for classes, methods, and so on.
We do this because so-and-so-I-forget\needcite,
but also it leaves our technique language-independent.
Special characters such as braces and semicolons will be removed during
preprocessing.

To extract text from the changesets, we look at the output of viewing
the \texttt{git diff} between two commits.
Figure~\ref{fig:diff} shows an example of what a changeset might look
like in Git.
In our changeset text extractor, we only extract text from removed or added lines.
Context and metadata lines are ignored.
Figure~\ref{fig:diffdocument} shows what words would be extracted to make up the document without preprocessing
from the change shown in Figure~\ref{fig:diff}.
Note that we do not consider what type of document the text originates from,
only that it is text changed by the commit.

Our document preprocessor implements the transformations described in Section~\ref{sec:preprocessing}.
%We filter \texttt{java.lang} class names before splitting tokens.
We split tokens based on camel case, underscores, and non-letters.
We normalize to lower case before filtering English stop words~\cite{StopWords}, Java keywords, and words shorter than three characters.
We do not stem words.


For our model generation, we use the open source Python library Gensim~\cite{Gensim}.
Gensim's LDA implementation is based on the Online LDA by Hoffman et al.~\cite{Hoffman-etal:2010}
and uses variational inference instead of a Collapsed Gibbs Sampler.
Unlike Gibbs sampling, in order to ensure that the model converges for each document,
we allow LDA to see each document $10$ times by setting Gensim's intialization parameter \texttt{passes} to this value.
We set the following LDA parameters for all four systems:
$100$ topics ($K$),
a symmetric $\alpha=0.01$,
$\beta$ is left as a default value of $1/K$ (also $0.01$).


\begin{figure}[ht]
\centering
\footnotesize
\begin{lstlisting}[language=diff, basicstyle=\ttfamily]
diff --git a/lao b/tzu
index 635ef2c..5af88a8 100644
--- a/lao
+++ b/tzu
@@ -1,7 +1,6 @@
-The Way that can be told of is not the eternal Way;
-The name that can be named is not the eternal name.
 The Nameless is the origin of Heaven and Earth;
-The Named is the mother of all things.
+The named is the mother of all things.
+
 Therefore let there always be non-being,
   so we may see their subtlety,
 And let there always be being,
@@ -9,3 +8,6 @@ And let there always be being,
 The two are the same,
 But after they are produced,
   they have different names.
+They both may be called deep and profound.
+Deeper and more profound,
+The door of all subtleties!
\end{lstlisting}
\caption{Example of a \texttt{git diff}. Black or blue lines denote metadata about the change, red lines (beginning with a single \texttt{-}) denote line removals, and green lines (beginning with a single \texttt{+}) denote line additions.}
\label{fig:diff}
\end{figure}

\begin{figure}[ht]
\em
\footnotesize
The Way that can be told of is not the eternal Way;
The name that can be named is not the eternal name.
The Named is the mother of all things.
The named is the mother of all things.
They both may be called deep and profound.
Deeper and more profound,
The door of all subtleties!
\caption{An example extracted changeset document before preprocessing}
\label{fig:diffdocument}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Data Collection and Analysis}

We create two corpora for each of our four subject systems.
We then used LDA to model the documents into topics.

To answer RQ1, we investigate the term frequency in each corpus.
We build two word-vectors from all unique terms from both corpora.
That is, each vector is of the same length and contain zero values for terms not in its respective corpus.
We measure the similarity of the two word-vectors using cosine similarity.


To answer RQ2, we follow Thomas et al~\cite{Thomas-etal:2011}.
We use topic distinctiveness to evaluate our topic models.
Distinct topics are topics with dissimilar word vectors to all other topics in the model.
Thomas et al.\ define topic distinctiveness (TD) of a topic $z_i$ as the mean
Kullback-Leibler (KL) divergence between the vectors $z_i$ and $z_j$, $\forall j \neq i$:

\begin{equation}
TD(\phi_{z_i}) =
\frac{1}{K - 1}
\sum_{j=1,j \neq i}^{K}
KL(\phi_{z_i}, \phi_{z_j})
\label{eq:topicdistinctiveness}
\end{equation}

For each model, we calculate the topic distinctiveness of every topic.
We score the overall topic distinctiveness of a model $\phi$ as the mean of
its topic's distinctiveness scores.

\begin{equation}
TD(\phi) =
\frac{1}{K - 1}
\sum_{i=1}^{K}
TD(\phi_{z_i})
\label{eq:overalltopicdistinctiveness}
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Results}

Butts
