% vim:syntax=tex

In this section we describe the design of a case study in which we
explore the relationship between ownership and linguistic topics in source code.
We describe the case study using the Goal-Question-Metric approach~\cite{Basili-etal:94}.
%The data for the case study is available in this paper's online
%appendix\footnote{\url{http://software.eng.ua.edu/data/ownership-topics}}.

\subsection{Definition and Context}

Our \textit{goal} is to explore the relationship between changeset topics and snapshot topics.
The \textit{quality focus} of the study is on informing development decisions and policy changes
that could lead to software with fewer defects.
The \textit{perspective} of the study is of a researcher, developer, or project manager who wishes
to gain understanding of the concepts or features implemented in the source code.
The \textit{context} of the study spans the version histories of 4 open source systems.

Toward achievement of our goal, we pose the following research questions:
\begin{enumerate}
    \item[]\hspace*{-20pt}\textit{RQ1: Do changeset- and snapshot-based corpora express the same terms?}
    \item[]\hspace*{-20pt}\textit{RQ2: Are topic models trained on changesets more distinct than topic models trained on a snapshot?}
\end{enumerate}
Basically, we want to know whether topic modeling changesets is as good as, or better than, topic modeling a snapshot.

In the remainder of this section we introduce the subjects of our study,
describe the setting of our study, and report our data collection and analysis procedures.

\subsection{Subject systems}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Study setup}

Rough outline of methodology

Gather corpora.
Get words from corpora.
Filter the words by splitting compound words and removing stop words.
Do not stem words because Biggers and Kraft found that the null stemmer was just as effective as six other stemmers in feature location techniques using LDA~\cite{Biggers-Kraft:2012}
Put filtered words from change sets into gensim to create topic models.
Put filtered words from release method into gensim to create topic models.
Test both types of models for topic distinctness.
Analyze effectiveness based on topic distinctness.


"Traditional metrics are, indeed, negatively correlated with the
measures of topic quality developed in this paper.  Our measures enable
new forms of model selection and suggest that practitioners developing
topic models should thus focus on evaluations that depend on real-world
task performance rather than optimizing likelihood-based measures." ~\cite{Chang-etal:2009}

\begin{figure}[ht]
\centering
\footnotesize
\begin{lstlisting}[language=diff, basicstyle=\ttfamily]
diff --git a/lao b/tzu
index 635ef2c..5af88a8 100644
--- a/lao
+++ b/tzu
@@ -1,7 +1,6 @@
-The Way that can be told of is not the eternal Way;
-The name that can be named is not the eternal name.
 The Nameless is the origin of Heaven and Earth;
-The Named is the mother of all things.
+The named is the mother of all things.
+
 Therefore let there always be non-being,
   so we may see their subtlety,
 And let there always be being,
@@ -9,3 +8,6 @@ And let there always be being,
 The two are the same,
 But after they are produced,
   they have different names.
+They both may be called deep and profound.
+Deeper and more profound,
+The door of all subtleties!
\end{lstlisting}
\caption{Example of a \texttt{git diff}. Black or blue lines denote metadata about the change, red lines (beginning with a single \texttt{-}) denote line removals, and green lines (beginning with a single \texttt{+}) denote line additions.}
\label{fig:diff}
\end{figure}

\begin{figure}[ht]
\em
\footnotesize
The Way that can be told of is not the eternal Way;
The name that can be named is not the eternal name.
The Named is the mother of all things.
The named is the mother of all things.
They both may be called deep and profound.
Deeper and more profound,
The door of all subtleties!
\caption{An example extracted changeset document before preprocessing}
\label{fig:diffdocument}
\end{figure}

\subsubsection{Text extraction from a release}

For our text extraction step on a release,
we simply use the entire contents of the document.
Unlike previous methods\needcite,
we do not parse the source code documents for classes, methods, and so on.
We do this because so-and-so-I-forget\needcite,
but also it leaves our technique language-independent.
Special characters such as braces and semicolons will be removed during
preprocessing.

\subsubsection{Text extraction from changesets}


To extract text from the changesets, we look at the output of viewing
the \texttt{git diff} between two commits.
Figure~\ref{fig:diff} shows an example of what a changeset might look
like in Git.

In our changeset text extractor, we only extract text from removed or added lines.
Context and metadata lines are ignored.
Figure~\ref{fig:diffdocument} shows what words would be extracted to make up the document without preprocessing
from the change shown in Figure~\ref{fig:diff}.
Note that we do not consider what type of document the text originates from,
only that it is text changed by the commit.

\subsubsection{Modeling}

For our topic modeling, we use the open source Python library Gensim~\cite{Gensim}.
Since Gensim's LDA implementation is based on the
Online LDA by Hoffman et al.~\cite{Hoffman-etal:2010},
it uses variational inference instead of a Collapsed Gibbs Sampler.
In order to ensure that the model converges for each document,
we allow LDA to see each document $10$ times by setting
Gensim's intialization parameter \texttt{passes} to this value.
We set the remaining LDA parameters as follows:
$100$ topics ($K$),
a symmetric $\alpha=0.01$,
$\beta$ is left as a default value of $1/K$ (also $0.01$).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Evaluation measures}

Following Thomas et al~\cite{Thomas-etal:2011}, we use topic distinctiveness
to evaluate our topic models.
Distinct topics are topics with dissimilar word vectors to all other topics in the model.
Thomas et al define topic distinctiveness (TD) of a topic $z_i$ as the mean
Kullback-Leibler (KL) divergence between the vectors $z_i$ and $z_j$, $\forall j \neq i$:

\begin{equation}
TD(\phi_{z_i}) = 
\frac{1}{K - 1}
\sum_{j=1,j \neq i}^{K}
KL(\phi_{z_i}, \phi_{z_j})
\label{eq:topicdistinctiveness}
\end{equation}

